# BLIP: Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding and Generation

**Authors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi**

 ## Overview
### Context & Problem Statement

Vision-language pretraining (VLP) has driven significant progress in multimodal learning, enabling models to connect images and text. However, existing pre-trained models face key limitations:
1. **Task Specialization Problem**: Most models excel at either understanding-based tasks (e.g., image-text retrieval) or generation-based tasks (e.g., image captioning), but not both.
2.	**Noisy Data Problem**: Many VLP models rely on web-crawled image-text pairs, which are often inaccurate or misaligned, making learning inefficient.

These challenges limit the effectiveness of vision-language models in real-world applications, where both understanding and generation capabilities are crucial.

 
